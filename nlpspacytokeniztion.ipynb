{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e72766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61299bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d038cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('''\"Let's go to N.Y!\"''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d786983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "for i in doc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a0e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Dr. strange loves pav bhaji of mumbai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60edfc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b42c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f974ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tony gave two $ to peter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10bf7642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d348df05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77817565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1290e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[2]\n",
    "token2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee07169a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "281153ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "token3 = doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72079937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "352fefb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "016ddf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index 0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gave ==> index 1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "two ==> index 2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index 3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index 4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "peter ==> index 5 is_alpha: True is_punct: False like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,\"==>\",\"index\",token.i,\n",
    "         \"is_alpha:\",token.is_alpha,\n",
    "         \"is_punct:\",token.is_punct,\n",
    "         \"like_num:\",token.like_num,\n",
    "         \"is_currency:\",token.is_currency)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f57b19e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (r\"C:\\Users\\shashank\\Downloads\\students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4224b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1c08af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3787e78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c540fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f6a6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6486ee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gimme double cheese extra large healthy pizza"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80e29382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[\n",
    "    {ORTH : \"gim\"},\n",
    "    {ORTH : \"me\"}\n",
    "])\n",
    "\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e431546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[\n",
    "    {ORTH : \"gim\"},\n",
    "    {ORTH : \"me\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e041069",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sents \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sents)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\tokens\\doc.pyx:924\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "\n",
    "for sents in doc.sents:\n",
    "    print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29c3e214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ae6a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1a1a8a032d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a931032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b50837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "\n",
    "for sents in doc.sents:\n",
    "    print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f99135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "568620bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3238bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12b05cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[http://www.data.gov/,\n",
       " http://www.science,\n",
       " http://data.gov.uk/.,\n",
       " http://www3.norc.org/gss+website/,\n",
       " http://www.europeansocialsurvey.org/.]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = []\n",
    "for i in doc:\n",
    "    if i.like_url:\n",
    "        url.append(i)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be151ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3592573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fedfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c28576",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "ans = []\n",
    "while i < len(doc)-1:\n",
    "    if(doc[i].like_num and doc[i+1].is_curreny):\n",
    "        s=doc[i].text+doc[i+1].text\n",
    "        ans.append(s)\n",
    "        print(i)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f625e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
